{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fceed5b-18b2-4047-a4ac-3dfb9f4755f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-39.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tzdata in /opt/conda/envs/py312/lib/python3.12/site-packages (from faker) (2025.3)\n",
      "Downloading faker-39.0.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faker\n",
      "Successfully installed faker-39.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bc8a80-8437-491e-8b37-f59e28981be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get the accessKey and secretKey from Environment\n",
    "accessKey = os.environ['AWS_ACCESS_KEY_ID']\n",
    "secretKey = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# point to mesos master or zookeeper entry (e.g., zk://10.10.10.10:2181/mesos)\n",
    "conf.setMaster(\"spark://spark-master:7077\")\n",
    "\n",
    "# set other options as desired\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.core.connection.ack.wait.timeout\", \"1200\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio-1:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", accessKey)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", secretKey)\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "conf.set(\"spark.sql.catalog.hive\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://hive-metastore:9083\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog.warehouse\", \"s3a://admin-bucket/iceberg/warehouse\")\n",
    "conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.0\")\n",
    "conf.set(\"spark.sql.legacy.allowNonEmptyLocationInCTAS\",\"true\")\n",
    "conf.set(\"spark.sql.hive.metastore.jars\",\"builtin\")\n",
    "\n",
    "spark = SparkSession.builder.appName('Jupyter').config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3063a7-d085-45b8-aa31-c107f0785360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4189f94d-ed63-4265-ae2c-4d38f6e5ae78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;SparkSession&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'SparkSession'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color: green\">1 rows affected.</span>"
      ],
      "text/plain": [
       "1 rows affected."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Field 1</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>spark_catalog</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+\n",
       "|    Field 1    |\n",
       "+---------------+\n",
       "| spark_catalog |\n",
       "+---------------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SHOW CATALOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5241b19b-0024-44e4-901b-29757936b2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;SparkSession&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'SparkSession'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS raw_person_xl;\n",
    "\n",
    "CREATE TABLE\n",
    "  IF NOT EXISTS raw_person_xl (\n",
    "    surrogate_key STRING,\n",
    "    person_id STRING,\n",
    "    -- identity\n",
    "    salutation STRING,\n",
    "    title STRING,\n",
    "    first_name STRING,\n",
    "    middle_name STRING,\n",
    "    last_name STRING,\n",
    "    suffix STRING,\n",
    "    gender STRING,\n",
    "    -- contact\n",
    "    email STRING,\n",
    "    phone_mobile STRING,\n",
    "    phone_home STRING,\n",
    "    -- address\n",
    "    street STRING,\n",
    "    house_number STRING,\n",
    "    postal_code STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    country STRING,\n",
    "    -- personal\n",
    "    birth_date DATE,\n",
    "    nationality STRING,\n",
    "    marital_status STRING,\n",
    "    number_of_children INT,\n",
    "    -- employment\n",
    "    employment_status STRING,\n",
    "    job_title STRING,\n",
    "    employer STRING,\n",
    "    annual_income DOUBLE,\n",
    "    -- identifiers\n",
    "    national_id STRING,\n",
    "    tax_id STRING,\n",
    "    -- metadata\n",
    "    source_system STRING,\n",
    "    status STRING,\n",
    "    export_date DATE,\n",
    "    load_ts TIMESTAMP\n",
    "  ) USING iceberg PARTITIONED BY (days (export_date)) LOCATION 's3a://warehouse-bucket/warehouse/raw_person_xl';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd613e6-7fb6-4e9a-b1d0-299492ca1ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;SparkSession&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'SparkSession'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dim_person_xl;\n",
    "\n",
    "CREATE TABLE\n",
    "  IF NOT EXISTS dim_person_xl (\n",
    "    surrogate_key STRING,\n",
    "    person_id STRING,\n",
    "    -- identity\n",
    "    salutation STRING,\n",
    "    title STRING,\n",
    "    first_name STRING,\n",
    "    middle_name STRING,\n",
    "    last_name STRING,\n",
    "    suffix STRING,\n",
    "    gender STRING,\n",
    "    -- contact\n",
    "    email STRING,\n",
    "    phone_mobile STRING,\n",
    "    phone_home STRING,\n",
    "    -- address\n",
    "    street STRING,\n",
    "    house_number STRING,\n",
    "    postal_code STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    country STRING,\n",
    "    -- personal\n",
    "    birth_date DATE,\n",
    "    nationality STRING,\n",
    "    marital_status STRING,\n",
    "    number_of_children INT,\n",
    "    -- employment\n",
    "    employment_status STRING,\n",
    "    job_title STRING,\n",
    "    employer STRING,\n",
    "    annual_income DOUBLE,\n",
    "    -- identifiers\n",
    "    national_id STRING,\n",
    "    tax_id STRING,\n",
    "    -- metadata\n",
    "    source_system STRING,\n",
    "    record_status STRING,\n",
    "    -- SCD2 metadata columns\n",
    "    valid_from TIMESTAMP,\n",
    "    valid_to TIMESTAMP,\n",
    "    is_current_version BOOLEAN,\n",
    "    is_active BOOLEAN,\n",
    "    source_loaded_at TIMESTAMP,\n",
    "    created_at TIMESTAMP,\n",
    "    replaced_at TIMESTAMP,\n",
    "    -- Additional metadata\n",
    "    change_type STRING,\n",
    "    record_hash STRING\n",
    "  ) USING iceberg PARTITIONED BY (days (valid_from), is_current_version) LOCATION 's3a://warehouse-bucket/warehouse/dim_person_xl';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5753132b-95ba-4acf-b0f1-d96c15a0efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import date, timedelta\n",
    "from faker import Faker\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a2f91a-fdfb-409e-b41c-5a04f6a0e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"surrogate_key\", StringType()),\n",
    "    StructField(\"person_id\", StringType()),\n",
    "\n",
    "    StructField(\"salutation\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"middle_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"suffix\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"phone_mobile\", StringType()),\n",
    "    StructField(\"phone_home\", StringType()),\n",
    "\n",
    "    StructField(\"street\", StringType()),\n",
    "    StructField(\"house_number\", StringType()),\n",
    "    StructField(\"postal_code\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"state\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "\n",
    "    StructField(\"birth_date\", DateType()),\n",
    "    StructField(\"nationality\", StringType()),\n",
    "    StructField(\"marital_status\", StringType()),\n",
    "    StructField(\"number_of_children\", IntegerType()),\n",
    "\n",
    "    StructField(\"employment_status\", StringType()),\n",
    "    StructField(\"job_title\", StringType()),\n",
    "    StructField(\"employer\", StringType()),\n",
    "    StructField(\"annual_income\", DoubleType()),\n",
    "\n",
    "    StructField(\"national_id\", StringType()),\n",
    "    StructField(\"tax_id\", StringType()),\n",
    "\n",
    "    StructField(\"source_system\", StringType()),\n",
    "    StructField(\"status\", StringType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b551511b-f7ab-49b7-bae8-296ed68e0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "def generate_person_row(person_id: int):\n",
    "    return (\n",
    "        str(uuid.uuid4()),\n",
    "        str(person_id),\n",
    "        random.choice([\"Mr\", \"Ms\", \"Mrs\", \"Dr\"]),\n",
    "        random.choice([\"\", \"Dr\", \"Prof\"]),\n",
    "        fake.first_name(),\n",
    "        fake.first_name() if random.random() < 0.3 else None,\n",
    "        fake.last_name(),\n",
    "        random.choice([\"\", \"Jr\", \"Sr\"]),\n",
    "        random.choice([\"M\", \"Fi\", \"X\"]),\n",
    "\n",
    "        fake.email(),\n",
    "        fake.phone_number(),\n",
    "        fake.phone_number(),\n",
    "\n",
    "        fake.street_name(),\n",
    "        str(fake.building_number()),\n",
    "        fake.postcode(),\n",
    "        fake.city(),\n",
    "        fake.state(),\n",
    "        fake.country_code(),\n",
    "\n",
    "        fake.date_of_birth(minimum_age=18, maximum_age=90),\n",
    "        fake.country_code(),\n",
    "        random.choice([\"single\", \"married\", \"divorced\", \"widowed\"]),\n",
    "        random.randint(0, 4),\n",
    "\n",
    "        random.choice([\"employed\", \"self-employed\", \"unemployed\", \"retired\"]),\n",
    "        fake.job(),\n",
    "        fake.company(),\n",
    "        round(random.uniform(30_000, 180_000), 2),\n",
    "\n",
    "        fake.ssn(),\n",
    "        fake.bothify(\"??######\"),\n",
    "\n",
    "        random.choice([\"CRM\", \"ERP\", \"HR\"]),\n",
    "        \"ACTIVE\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e26995-1a40-4f9d-9c9f-8eed5a4e1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PERSONS = 200_000   # scale here\n",
    "\n",
    "rows = [generate_person_row(i) for i in range(INITIAL_PERSONS)]\n",
    "df = spark.createDataFrame(rows, schema).cache()\n",
    "next_person_id = INITIAL_PERSONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f0f4a20-23ad-440e-9d7b-556d795adb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_RATE = 0.05\n",
    "INSERT_RATE = 0.01\n",
    "DELETE_RATE = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39049a49-8ce8-4175-ab69-802fb9240b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_daily_changes(df, next_person_id):\n",
    "    updates = df.sample(UPDATE_RATE)\n",
    "    deletes = df.sample(DELETE_RATE)\n",
    "\n",
    "    updated = (\n",
    "        updates\n",
    "        .withColumn(\"email\", F.when(F.rand() < 0.6, fake.email()).otherwise(F.col(\"email\")))\n",
    "        .withColumn(\"street\", F.when(F.rand() < 0.4, fake.street_name()).otherwise(F.col(\"street\")))\n",
    "        .withColumn(\"job_title\", F.when(F.rand() < 0.3, fake.job()).otherwise(F.col(\"job_title\")))\n",
    "        .withColumn(\n",
    "            \"annual_income\",\n",
    "            F.when(F.rand() < 0.3, F.col(\"annual_income\") * (1 + (F.rand() - 0.5) / 5))\n",
    "             .otherwise(F.col(\"annual_income\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    remaining = df.subtract(deletes).subtract(updates)\n",
    "\n",
    "    inserts_count = int(df.count() * INSERT_RATE)\n",
    "    new_rows = [\n",
    "        generate_person_row(next_person_id + i)\n",
    "        for i in range(inserts_count)\n",
    "    ]\n",
    "\n",
    "    inserts = spark.createDataFrame(new_rows, schema)\n",
    "\n",
    "    full_export = remaining.unionByName(updated).unionByName(inserts)\n",
    "\n",
    "    return full_export, next_person_id + inserts_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c706ed-2223-4e87-b17b-e1b215733950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 | rows=201081 | next_person_id=202000\n",
      "2024-01-02 | rows=202124 | next_person_id=204010\n",
      "2024-01-03 | rows=203216 | next_person_id=206031\n",
      "2024-01-04 | rows=204274 | next_person_id=208063\n",
      "2024-01-05 | rows=205312 | next_person_id=210105\n",
      "2024-01-06 | rows=206357 | next_person_id=212158\n",
      "2024-01-07 | rows=207455 | next_person_id=214221\n",
      "2024-01-08 | rows=208536 | next_person_id=216295\n",
      "2024-01-09 | rows=209642 | next_person_id=218380\n",
      "2024-01-10 | rows=210727 | next_person_id=220476\n",
      "2024-01-11 | rows=211855 | next_person_id=222583\n",
      "2024-01-12 | rows=212988 | next_person_id=224701\n",
      "2024-01-13 | rows=214085 | next_person_id=226830\n",
      "2024-01-14 | rows=215220 | next_person_id=228970\n",
      "2024-01-15 | rows=216283 | next_person_id=231122\n",
      "2024-01-16 | rows=217439 | next_person_id=233284\n",
      "2024-01-17 | rows=218600 | next_person_id=235458\n",
      "2024-01-18 | rows=219753 | next_person_id=237644\n",
      "2024-01-19 | rows=220898 | next_person_id=239841\n",
      "2024-01-20 | rows=222085 | next_person_id=242049\n",
      "2024-01-21 | rows=223235 | next_person_id=244269\n",
      "2024-01-22 | rows=224371 | next_person_id=246501\n",
      "2024-01-23 | rows=225552 | next_person_id=248744\n",
      "2024-01-24 | rows=226729 | next_person_id=250999\n",
      "2024-01-25 | rows=227917 | next_person_id=253266\n",
      "2024-01-26 | rows=229129 | next_person_id=255545\n",
      "2024-01-27 | rows=230306 | next_person_id=257836\n",
      "2024-01-28 | rows=231460 | next_person_id=260139\n",
      "2024-01-29 | rows=232627 | next_person_id=262453\n",
      "2024-01-30 | rows=233860 | next_person_id=264779\n"
     ]
    }
   ],
   "source": [
    "start_date = date(2024, 1, 1)\n",
    "DAYS = 30\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"s3a://admin-bucket/checkpoints\")\n",
    "\n",
    "for d in range(DAYS):\n",
    "    export_date = start_date + timedelta(days=d)\n",
    "\n",
    "    df, next_person_id = apply_daily_changes(df, next_person_id)\n",
    "\n",
    "    out_df = (\n",
    "        df\n",
    "        .withColumn(\"export_date\", F.lit(export_date))\n",
    "        .withColumn(\"load_ts\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    out_df.writeTo(\"raw_person_xl\").overwrite(F.expr(f\"export_date = DATE '{export_date}'\"))\n",
    "\n",
    "    # üîë break lineage\n",
    "    df = df.checkpoint(eager=True)\n",
    "    \n",
    "    spark.catalog.clearCache()\n",
    "\n",
    "    print(f\"{export_date} | rows={out_df.count()} | next_person_id={next_person_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024eb5b-f992-40fe-814e-b9b094f74e65",
   "metadata": {},
   "source": [
    "## SCD2 Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449f96d8-9d88-498c-8d2d-84352ad1664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"salutation\",\n",
    "    \"title\",\n",
    "    \"first_name\",\n",
    "    \"middle_name\",\n",
    "    \"last_name\",\n",
    "    \"suffix\",\n",
    "    \"gender\",\n",
    "    \"email\",\n",
    "    \"phone_mobile\",\n",
    "    \"phone_home\",\n",
    "    \"street\",\n",
    "    \"house_number\",\n",
    "    \"postal_code\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"country\",\n",
    "    \"birth_date\",\n",
    "    \"nationality\",\n",
    "    \"marital_status\",\n",
    "    \"number_of_children\",\n",
    "    \"employment_status\",\n",
    "    \"job_title\",\n",
    "    \"employer\",\n",
    "    \"annual_income\",\n",
    "    \"national_id\",\n",
    "    \"tax_id\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fb45865-917e-49ab-8961-37fb67ef8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_values(values, prefix):\n",
    "    return \", \".join(f\"{prefix}{v}\" for v in values)\n",
    "\n",
    "def format_sql(load_date: str, current_timestamp: str, table_name: str, pk_col: str, val_columns: []):\n",
    "    val_columns_str = format_values(val_columns, \"\")\n",
    "    source_val_columns_str = format_values(val_columns, \"source.\")\n",
    "    \n",
    "    stmt = f\"\"\"\n",
    "    WITH changed_records AS (\n",
    "        SELECT \n",
    "            src.*,\n",
    "            CASE \n",
    "                WHEN tgt.{pk_col} IS NULL THEN 'NEW'\n",
    "                WHEN src.row_hash != tgt.row_hash THEN 'CHANGED'\n",
    "                ELSE 'UNCHANGED'\n",
    "            END AS change_classification\n",
    "        FROM (\n",
    "        \tSELECT *,\n",
    "     \t\t\tsha2(concat_ws('||', {val_columns_str}, status), 256) AS row_hash\n",
    "  \t\t\tFROM raw_person_xl\n",
    "        ) src\n",
    "        LEFT JOIN (\n",
    "            SELECT \n",
    "                surrogate_key,\n",
    "                {pk_col},\n",
    "                sha2(concat_ws('||', {pk_col}, {val_columns_str}, CASE WHEN is_active THEN 'ACTIVE' else 'INACTIVE' END), 256) AS row_hash,\n",
    "                source_loaded_at,\n",
    "                valid_from\n",
    "            FROM dim_person_xl\n",
    "            WHERE is_current_version = true\n",
    "        ) tgt ON src.{pk_col} = tgt.{pk_col}\n",
    "        WHERE src.export_date = CAST('{load_date}' as date)\n",
    "    ),\n",
    "    records_to_process AS (\n",
    "        SELECT *\n",
    "        FROM changed_records\n",
    "        WHERE change_classification IN ('NEW', 'CHANGED')\n",
    "    ),\n",
    "    prepared_source AS (\n",
    "        -- Original records for matching existing rows (updates)\n",
    "        SELECT\n",
    "            surrogate_key,\n",
    "            {pk_col} AS merge_key,  -- Used for matching\n",
    "            {pk_col},\n",
    "            {val_columns_str},\n",
    "            export_date ,\n",
    "            status,\n",
    "            'UPDATE_EXISTING' AS operation_type\n",
    "        FROM records_to_process\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        -- Duplicate records with NULL key for insertions\n",
    "        SELECT \n",
    "            surrogate_key,\n",
    "            NULL AS merge_key,     -- NULL prevents matching, forces insert\n",
    "            {pk_col},\n",
    "            {val_columns_str},\n",
    "            export_date,\n",
    "            status,\n",
    "            'INSERT_NEW_VERSION' AS operation_type\n",
    "        FROM records_to_process\n",
    "        WHERE change_classification = 'CHANGED'  -- Only for updates, not new records\n",
    "    )\n",
    "    \n",
    "    MERGE INTO dim_person_xl target\n",
    "    USING prepared_source source\n",
    "    ON target.{pk_col} = source.merge_key \n",
    "       AND target.is_current_version = true\n",
    "    -- Close existing current records for updated entities\n",
    "    WHEN MATCHED \n",
    "        AND source.operation_type = 'UPDATE_EXISTING'\n",
    "        AND source.export_date > target.source_loaded_at\n",
    "    THEN UPDATE SET\n",
    "        valid_to = source.export_date - INTERVAL 1 SECOND,\n",
    "        is_current_version = false,\n",
    "        change_type = 'SUPERSEDED',\n",
    "        created_at = CAST('{current_timestamp}' AS TIMESTAMP)\n",
    "    -- Insert new records (both new entities and new versions)\n",
    "    WHEN NOT MATCHED \n",
    "    THEN INSERT (\n",
    "        surrogate_key,\n",
    "        {pk_col},\n",
    "        {val_columns_str},\n",
    "        valid_from,\n",
    "        valid_to,\n",
    "        is_current_version,\n",
    "        is_active,\n",
    "        source_loaded_at,\n",
    "        created_at,\n",
    "        replaced_at,\n",
    "        change_type,\n",
    "        record_hash\n",
    "    ) VALUES (\n",
    "        source.surrogate_key,\n",
    "        source.{pk_col},\n",
    "        {source_val_columns_str},\n",
    "        source.export_date,\n",
    "        CAST('9999-12-31 23:59:59' AS TIMESTAMP),  -- Far future date\n",
    "        true,\n",
    "        CASE WHEN source.status = 'ACTIVE' THEN true ELSE false END,\n",
    "        source.export_date,\n",
    "        CAST('{current_timestamp}' AS TIMESTAMP),\n",
    "        CAST('9999-12-31 23:59:59' AS TIMESTAMP),  -- Far future date\n",
    "        CASE \n",
    "            WHEN source.operation_type = 'UPDATE_EXISTING' THEN 'NEW'\n",
    "            ELSE 'SUPERSEDED_BY'\n",
    "        END,\n",
    "        sha2(concat_ws('|', \n",
    "            cast(source.{pk_col} as string),\n",
    "            {source_val_columns_str},status\n",
    "        ), 256)\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    return stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34efe3ea-e961-440e-be59-a97e564ccb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    WITH changed_records AS (\n",
      "        SELECT \n",
      "            src.*,\n",
      "            CASE \n",
      "                WHEN tgt.person_id IS NULL THEN 'NEW'\n",
      "                WHEN src.row_hash != tgt.row_hash THEN 'CHANGED'\n",
      "                ELSE 'UNCHANGED'\n",
      "            END AS change_classification\n",
      "        FROM (\n",
      "        \tSELECT *,\n",
      "     \t\t\tsha2(concat_ws('||', salutation, title, first_name, middle_name, last_name, suffix, gender, email, phone_mobile, phone_home, street, house_number, postal_code, city, state, country, birth_date, nationality, marital_status, number_of_children, employment_status, job_title, employer, annual_income, national_id, tax_id, status), 256) AS row_hash\n",
      "  \t\t\tFROM raw_person_xl\n",
      "        ) src\n",
      "        LEFT JOIN (\n",
      "            SELECT \n",
      "                surrogate_key,\n",
      "                person_id,\n",
      "                sha2(concat_ws('||', person_id, salutation, title, first_name, middle_name, last_name, suffix, gender, email, phone_mobile, phone_home, street, house_number, postal_code, city, state, country, birth_date, nationality, marital_status, number_of_children, employment_status, job_title, employer, annual_income, national_id, tax_id, CASE WHEN is_active THEN 'ACTIVE' else 'INACTIVE' END), 256) AS row_hash,\n",
      "                source_loaded_at,\n",
      "                valid_from\n",
      "            FROM dim_person_xl\n",
      "            WHERE is_current_version = true\n",
      "        ) tgt ON src.person_id = tgt.person_id\n",
      "        WHERE src.export_date = CAST('2024-01-02' as date)\n",
      "    ),\n",
      "    records_to_process AS (\n",
      "        SELECT *\n",
      "        FROM changed_records\n",
      "        WHERE change_classification IN ('NEW', 'CHANGED')\n",
      "    ),\n",
      "    prepared_source AS (\n",
      "        -- Original records for matching existing rows (updates)\n",
      "        SELECT\n",
      "            surrogate_key,\n",
      "            person_id AS merge_key,  -- Used for matching\n",
      "            person_id,\n",
      "            salutation, title, first_name, middle_name, last_name, suffix, gender, email, phone_mobile, phone_home, street, house_number, postal_code, city, state, country, birth_date, nationality, marital_status, number_of_children, employment_status, job_title, employer, annual_income, national_id, tax_id,\n",
      "            export_date ,\n",
      "            status,\n",
      "            'UPDATE_EXISTING' AS operation_type\n",
      "        FROM records_to_process\n",
      "\n",
      "        UNION ALL\n",
      "\n",
      "        -- Duplicate records with NULL key for insertions\n",
      "        SELECT \n",
      "            surrogate_key,\n",
      "            NULL AS merge_key,     -- NULL prevents matching, forces insert\n",
      "            person_id,\n",
      "            salutation, title, first_name, middle_name, last_name, suffix, gender, email, phone_mobile, phone_home, street, house_number, postal_code, city, state, country, birth_date, nationality, marital_status, number_of_children, employment_status, job_title, employer, annual_income, national_id, tax_id,\n",
      "            export_date,\n",
      "            status,\n",
      "            'INSERT_NEW_VERSION' AS operation_type\n",
      "        FROM records_to_process\n",
      "        WHERE change_classification = 'CHANGED'  -- Only for updates, not new records\n",
      "    )\n",
      "\n",
      "    MERGE INTO dim_person_xl target\n",
      "    USING prepared_source source\n",
      "    ON target.person_id = source.merge_key \n",
      "       AND target.is_current_version = true\n",
      "    -- Close existing current records for updated entities\n",
      "    WHEN MATCHED \n",
      "        AND source.operation_type = 'UPDATE_EXISTING'\n",
      "        AND source.export_date > target.source_loaded_at\n",
      "    THEN UPDATE SET\n",
      "        valid_to = source.export_date - INTERVAL 1 SECOND,\n",
      "        is_current_version = false,\n",
      "        change_type = 'SUPERSEDED',\n",
      "        created_at = CAST('2025-01-07' AS TIMESTAMP)\n",
      "    -- Insert new records (both new entities and new versions)\n",
      "    WHEN NOT MATCHED \n",
      "    THEN INSERT (\n",
      "        surrogate_key,\n",
      "        person_id,\n",
      "        salutation, title, first_name, middle_name, last_name, suffix, gender, email, phone_mobile, phone_home, street, house_number, postal_code, city, state, country, birth_date, nationality, marital_status, number_of_children, employment_status, job_title, employer, annual_income, national_id, tax_id,\n",
      "        valid_from,\n",
      "        valid_to,\n",
      "        is_current_version,\n",
      "        is_active,\n",
      "        source_loaded_at,\n",
      "        created_at,\n",
      "        replaced_at,\n",
      "        change_type,\n",
      "        record_hash\n",
      "    ) VALUES (\n",
      "        source.surrogate_key,\n",
      "        source.person_id,\n",
      "        source.salutation, source.title, source.first_name, source.middle_name, source.last_name, source.suffix, source.gender, source.email, source.phone_mobile, source.phone_home, source.street, source.house_number, source.postal_code, source.city, source.state, source.country, source.birth_date, source.nationality, source.marital_status, source.number_of_children, source.employment_status, source.job_title, source.employer, source.annual_income, source.national_id, source.tax_id,\n",
      "        source.export_date,\n",
      "        CAST('9999-12-31 23:59:59' AS TIMESTAMP),  -- Far future date\n",
      "        true,\n",
      "        CASE WHEN source.status = 'ACTIVE' THEN true ELSE false END,\n",
      "        source.export_date,\n",
      "        CAST('2025-01-07' AS TIMESTAMP),\n",
      "        CAST('9999-12-31 23:59:59' AS TIMESTAMP),  -- Far future date\n",
      "        CASE \n",
      "            WHEN source.operation_type = 'UPDATE_EXISTING' THEN 'NEW'\n",
      "            ELSE 'SUPERSEDED_BY'\n",
      "        END,\n",
      "        sha2(concat_ws('|', \n",
      "            cast(source.person_id as string),\n",
      "            source.salutation, source.title, source.first_name, source.middle_name, source.last_name, source.suffix, source.gender, source.email, source.phone_mobile, source.phone_home, source.street, source.house_number, source.postal_code, source.city, source.state, source.country, source.birth_date, source.nationality, source.marital_status, source.number_of_children, source.employment_status, source.job_title, source.employer, source.annual_income, source.national_id, source.tax_id,status\n",
      "        ), 256)\n",
      "    )\n",
      "\n",
      "    \n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o66.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1519.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1519.0 (TID 49170) (192.168.148.17 executor 0): org.apache.spark.SparkRuntimeException: [MERGE_CARDINALITY_VIOLATION] The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table.\nThis could result in the target row being operated on more than once with an update or delete operation and is not allowed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.mergeCardinalityViolationError(QueryExecutionErrors.scala:2702)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$BitmapCardinalityValidator.validate(MergeRowsExec.scala:164)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:203)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:176)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:514)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkRuntimeException: [MERGE_CARDINALITY_VIOLATION] The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table.\nThis could result in the target row being operated on more than once with an update or delete operation and is not allowed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.mergeCardinalityViolationError(QueryExecutionErrors.scala:2702)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$BitmapCardinalityValidator.validate(MergeRowsExec.scala:164)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:203)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:176)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:514)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m stmt = format_sql(\u001b[33m'\u001b[39m\u001b[33m2024-01-02\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m2025-01-07\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mperson\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mperson_id\u001b[39m\u001b[33m'\u001b[39m, fields)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(stmt)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m spark.catalog.clearCache()\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py312/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py312/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py312/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/py312/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o66.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1519.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1519.0 (TID 49170) (192.168.148.17 executor 0): org.apache.spark.SparkRuntimeException: [MERGE_CARDINALITY_VIOLATION] The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table.\nThis could result in the target row being operated on more than once with an update or delete operation and is not allowed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.mergeCardinalityViolationError(QueryExecutionErrors.scala:2702)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$BitmapCardinalityValidator.validate(MergeRowsExec.scala:164)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:203)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:176)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:514)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkRuntimeException: [MERGE_CARDINALITY_VIOLATION] The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table.\nThis could result in the target row being operated on more than once with an update or delete operation and is not allowed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.mergeCardinalityViolationError(QueryExecutionErrors.scala:2702)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$BitmapCardinalityValidator.validate(MergeRowsExec.scala:164)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:203)\n\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.next(MergeRowsExec.scala:176)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:514)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "start_date = date(2024, 1, 1)\n",
    "DAYS = 1\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"s3a://admin-bucket/checkpoints\")\n",
    "\n",
    "for d in range(DAYS):\n",
    "    load_date = start_date + timedelta(days=d)\n",
    "\n",
    "    stmt = format_sql('2024-01-02', '2025-01-07', 'person', 'person_id', fields)\n",
    "    print(stmt)\n",
    "    spark.sql(stmt)\n",
    "        \n",
    "    spark.catalog.clearCache()\n",
    "\n",
    "    print(f\"{load_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629c847-79cd-423c-bfc2-7c912f774871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.8 (ipykernel)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
